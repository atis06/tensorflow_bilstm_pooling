{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis_net_poc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObZxb7ee2C1eXLdlQx3MKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atis06/tensorflow_bilstm_pooling/blob/master/Thesis_net_poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDaWthyR4EE2",
        "colab_type": "code",
        "outputId": "9cca4900-db0f-415f-b37b-4931ef4e8cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!wget https://github.com/amirziai/cse6242-project/raw/master/resources/datasets/bbc-text.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-14 09:36:32--  https://github.com/amirziai/cse6242-project/raw/master/resources/datasets/bbc-text.csv\n",
            "Resolving github.com (github.com)... 140.82.118.3\n",
            "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/amirziai/cse6242-project/master/resources/datasets/bbc-text.csv [following]\n",
            "--2020-02-14 09:36:33--  https://raw.githubusercontent.com/amirziai/cse6242-project/master/resources/datasets/bbc-text.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5057493 (4.8M) [text/plain]\n",
            "Saving to: ‘bbc-text.csv.2’\n",
            "\n",
            "bbc-text.csv.2      100%[===================>]   4.82M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-02-14 09:36:33 (123 MB/s) - ‘bbc-text.csv.2’ saved [5057493/5057493]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeH68YGY37hd",
        "colab_type": "code",
        "outputId": "4599fdbd-6b64-494a-e573-aab7de15b46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "vocab_size = 10000\n",
        "embedding_dim = 64\n",
        "max_length = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = 'OOV'\n",
        "training_portion = .8\n",
        "\n",
        "articles = []\n",
        "labels = []\n",
        "\n",
        "with open(\"bbc-text.csv\", 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        labels.append(row[0])\n",
        "        article = row[1]\n",
        "        for word in STOPWORDS:\n",
        "            token = ' ' + word + ' '\n",
        "            article = article.replace(token, ' ')\n",
        "            article = article.replace(' ', ' ')\n",
        "        articles.append(article)\n",
        "\n",
        "train_size = int(len(articles) * training_portion)\n",
        "\n",
        "train_articles = articles[0: train_size]\n",
        "train_labels = labels[0: train_size]\n",
        "\n",
        "validation_articles = articles[train_size:]\n",
        "validation_labels = labels[train_size:]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_articles)\n",
        "word_index = tokenizer.word_index\n",
        "dict(list(word_index.items())[0:10])\n",
        "\n",
        "\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
        "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_article(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_article(train_sequences[1]))\n",
        "print(decode_article(train_padded[1]))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "1.15.0\n",
            "worldcom boss left books alone former worldcom boss bernie ebbers accused overseeing 11bn £5 8bn fraud never made accounting decisions witness told jurors david myers made comments questioning defence lawyers arguing mr ebbers responsible worldcom problems phone company collapsed 2002 prosecutors claim losses hidden protect firm shares mr myers already pleaded guilty fraud assisting prosecutors monday defence lawyer reid weingarten tried distance client allegations cross examination asked mr myers ever knew mr ebbers make accounting decision aware mr myers replied ever know mr ebbers make accounting entry worldcom books mr weingarten pressed replied witness mr myers admitted ordered false accounting entries request former worldcom chief financial officer scott sullivan defence lawyers trying paint mr sullivan admitted fraud testify later trial OOV behind worldcom accounting house cards mr ebbers team meanwhile looking portray OOV boss admission OOV graduate economist whatever abilities mr ebbers transformed worldcom relative unknown OOV telecoms giant investor darling late 1990s worldcom problems mounted however competition increased telecoms boom OOV out firm finally collapsed shareholders lost 180bn 20 000 workers lost jobs mr ebbers trial expected last two months found guilty former ceo faces substantial jail sentence firmly declared innocence\n",
            "worldcom boss left books alone former worldcom boss bernie ebbers accused overseeing 11bn £5 8bn fraud never made accounting decisions witness told jurors david myers made comments questioning defence lawyers arguing mr ebbers responsible worldcom problems phone company collapsed 2002 prosecutors claim losses hidden protect firm shares mr myers already pleaded guilty fraud assisting prosecutors monday defence lawyer reid weingarten tried distance client allegations cross examination asked mr myers ever knew mr ebbers make accounting decision aware mr myers replied ever know mr ebbers make accounting entry worldcom books mr weingarten pressed replied witness mr myers admitted ordered false accounting entries request former worldcom chief financial officer scott sullivan defence lawyers trying paint mr sullivan admitted fraud testify later trial OOV behind worldcom accounting house cards mr ebbers team meanwhile looking portray OOV boss admission OOV graduate economist whatever abilities mr ebbers transformed worldcom relative unknown OOV telecoms giant investor darling late 1990s worldcom problems mounted however competition increased telecoms boom OOV out firm finally collapsed shareholders lost 180bn 20 000 workers lost jobs mr ebbers trial expected last two months found guilty former ceo faces substantial jail sentence firmly declared innocence ? ? ? ? ? ? ? ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cvnPLDe16hK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#W2V\n",
        "w2v_dim=300\n",
        "import gensim\n",
        "splitted_articles=[article.split() for article in articles]\n",
        "model = gensim.models.Word2Vec(iter=5, size=w2v_dim, workers=4,min_count=4) \n",
        "model.build_vocab(splitted_articles)\n",
        "model.train(splitted_articles, total_examples=len(splitted_articles), epochs = 15)\n",
        "\n",
        "vocabulary_size = len(model.wv.vocab)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJU0hnX1rtvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_articles_w2v = []\n",
        "\n",
        "for row in train_padded:\n",
        "  w2v_sentence = []\n",
        "  for word in enumerate(decode_article(row).split()):\n",
        "    try:\n",
        "      w2v_sentence.append(np.asarray(model.wv[word[1]]))\n",
        "    except Exception as e:\n",
        "      w2v_sentence.append(np.asarray(np.zeros(300)))\n",
        "  train_articles_w2v.append(np.asarray(w2v_sentence))\n",
        "\n",
        "#train_articles_w2v = np.asarray([elem[0:200] for elem in train_articles_w2v])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8c30WJX6ArF",
        "colab_type": "code",
        "outputId": "7785797d-66ec-474b-85fb-c10adb899aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def next_batch(batch_size):\n",
        "    idx = np.arange(0, len(train_articles_w2v))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[0:batch_size]\n",
        "    data_shuffle = [train_articles_w2v[i] for i in idx]\n",
        "    labels_shuffle = [training_label_seq[i] for i in idx]\n",
        "\n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
        "\n",
        "\n",
        "X_batch, y_batch = next_batch(40)\n",
        "print(X_batch)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 8.62099648e-01 -7.58275092e-01 -5.10803342e-01 ...  1.10714602e+00\n",
            "    7.02131867e-01  6.15425520e-02]\n",
            "  [-1.09876436e-03  7.00126141e-02 -2.32930705e-01 ...  4.53263193e-01\n",
            "    1.31419405e-01 -4.49505411e-02]\n",
            "  [ 1.49697095e-01  3.25755291e-02 -1.69385076e-01 ...  2.96258718e-01\n",
            "    2.56735742e-01 -1.86892077e-01]\n",
            "  ...\n",
            "  [ 7.75562525e-02 -3.08722910e-02 -1.60007313e-01 ...  2.17864543e-01\n",
            "    2.28700489e-01  2.54304614e-02]\n",
            "  [-1.74080038e+00  8.65813717e-02 -9.32320297e-01 ...  6.69865131e-01\n",
            "    1.62692010e+00 -7.63924047e-02]\n",
            "  [-1.49393833e+00  9.34744835e-01 -1.25198472e+00 ...  4.95028377e-01\n",
            "    7.29091242e-02  1.04749000e+00]]\n",
            "\n",
            " [[ 6.97790533e-02  8.23096707e-02  3.71163222e-03 ...  1.75992966e-01\n",
            "    8.14921334e-02 -5.02074957e-02]\n",
            "  [-3.89044613e-01  1.26834655e+00 -6.07722163e-01 ... -6.90266192e-01\n",
            "    1.29577294e-01 -1.95300543e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  ...\n",
            "  [ 3.43776017e-01  2.20502675e-01  4.72801812e-02 ...  4.48147297e-01\n",
            "    3.02574307e-01  1.80509552e-01]\n",
            "  [ 3.16315107e-02  5.96684888e-02 -5.68792447e-02 ...  2.06613958e-01\n",
            "    9.83740166e-02 -4.64150682e-02]\n",
            "  [ 5.32398522e-01  5.33629656e-01  4.30474281e-02 ... -1.31393611e+00\n",
            "    4.95786130e-01 -1.51225245e+00]]\n",
            "\n",
            " [[ 8.76346469e-01 -8.68130028e-02 -7.18129635e-01 ...  1.44633102e+00\n",
            "    4.27114785e-01 -3.90214980e-01]\n",
            "  [ 1.35809153e-01  4.65378426e-02 -1.69063717e-01 ...  4.14708197e-01\n",
            "    1.64903432e-01 -1.49120390e-01]\n",
            "  [ 5.13424039e-01 -3.50667745e-01  3.01874518e-01 ... -2.14052685e-02\n",
            "    7.13931322e-02  3.30171943e-01]\n",
            "  ...\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.26315698e-02  2.73739193e-02 -2.42649182e-03 ...  2.99250819e-02\n",
            "    7.94397816e-02  5.39840609e-02]\n",
            "  [-4.30997014e-01 -1.62342489e-01 -4.38327119e-02 ... -8.13635290e-02\n",
            "    7.57930636e-01  9.52189684e-01]\n",
            "  [ 7.36266142e-04  1.15658669e-02  9.74305421e-02 ...  8.47330168e-02\n",
            "    2.71209717e-01  4.63428736e-01]\n",
            "  ...\n",
            "  [-1.01848796e-01 -1.74488053e-02  6.60983548e-02 ...  2.08502412e-02\n",
            "    2.09412366e-01  2.00732470e-01]\n",
            "  [-3.79268676e-01  1.00850157e-01  4.20958921e-02 ...  1.31680310e-01\n",
            "    1.36346564e-01  4.39539194e-01]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[-1.31319389e-01  4.20829318e-02 -6.20777719e-02 ...  2.34602243e-01\n",
            "    1.00309141e-01 -6.64375499e-02]\n",
            "  [-2.32893497e-01 -6.70870245e-02  4.56833057e-02 ...  9.34210345e-02\n",
            "    1.35039225e-01  2.43043512e-01]\n",
            "  [-4.77549165e-01  4.10672069e-01  1.28316462e-01 ...  4.83654052e-01\n",
            "    2.02939540e-01 -4.78160307e-02]\n",
            "  ...\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[ 2.83409189e-02  1.77412793e-01 -1.01188540e-01 ...  2.33121932e-01\n",
            "    1.23367585e-01 -6.60341084e-02]\n",
            "  [-2.23094095e-02  8.41691047e-02 -8.20263550e-02 ...  1.32797599e-01\n",
            "    6.73260838e-02 -7.07162321e-02]\n",
            "  [-6.39091134e-01 -6.30012080e-02 -7.70682454e-01 ... -3.03401332e-02\n",
            "    4.31683362e-01 -1.09045637e+00]\n",
            "  ...\n",
            "  [-2.65822798e-01  1.37357578e-01 -1.41407847e-01 ...  3.94616932e-01\n",
            "    1.62876636e-01  2.22556684e-02]\n",
            "  [-6.96331859e-01  7.31973350e-01 -3.31192017e-01 ...  1.64210296e+00\n",
            "    3.91793400e-01  6.71383083e-01]\n",
            "  [-4.43447649e-01  2.58284092e-01  2.29264617e-01 ... -3.15080583e-01\n",
            "    9.77107063e-02  2.05463976e-01]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GARqFrnl_xDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_articles_w2v = []\n",
        "\n",
        "for row in validation_padded:\n",
        "  w2v_sentence = []\n",
        "  for word in enumerate(decode_article(row).split()):\n",
        "    try:\n",
        "      w2v_sentence.append(np.asarray(model.wv[word[1]]))\n",
        "    except Exception as e:\n",
        "      w2v_sentence.append(np.asarray(np.zeros(300)))\n",
        "  validation_articles_w2v.append(np.asarray(w2v_sentence))\n",
        "\n",
        "#train_articles_w2v = np.asarray([elem[0:200] for elem in train_articles_w2v])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9cQa6S-E8PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_shape(varname, var):\n",
        "  with tf.Session() as sess:\n",
        "    \"\"\"\n",
        "    :param varname: tensor name\n",
        "    :param var: tensor variable\n",
        "    \"\"\"\n",
        "    print('{0} : {1}'.format(varname, tf.shape(var, name=None)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9R-yMd04Miq",
        "colab_type": "code",
        "outputId": "b21c8241-9acb-42aa-b242-907dd0e6705c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "######################################################################\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import LSTMCell, Bidirectional\n",
        "\n",
        "\n",
        "# Just one feature, the time series(embeddig dim)\n",
        "num_inputs = 300\n",
        "# Num of steps in each batch (seqlength)\n",
        "num_time_steps = 200\n",
        "# 100 neuron layer, play with this\n",
        "num_hidden = 200\n",
        "# learning rate you can play with this\n",
        "learning_rate = 0.001\n",
        "# how many iterations to go through (training steps), you can play with this\n",
        "epochs = 200\n",
        "# Size of the batch of data\n",
        "batch_size = 100\n",
        "n_classes = 5\n",
        "dropout_keep_prob = 0.8\n",
        "\n",
        "'''with tf.Session() as sess:\n",
        "    print(tf.one_hot(y_batch, n_classes,\n",
        "                     axis=-1).eval())'''\n",
        "\n",
        "\n",
        "def RNN(x, weights, biases, timesteps, num_hidden):\n",
        "    x = tf.unstack(x, timesteps, 1)\n",
        "    rnn_cell = tf.contrib.rnn.BasicRNNCell(num_hidden)\n",
        "    states_series, current_state = tf.contrib.rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    return tf.matmul(states_series[-1], weights) + biases\n",
        "\n",
        "def BiRNN(x, weights, biases, timesteps, num_hidden):\n",
        "\n",
        "    #x = tf.unstack(x, timesteps, 1)\n",
        "    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "    if dropout_keep_prob is not None:\n",
        "            lstm_fw_cell=tf.contrib.rnn.DropoutWrapper(lstm_fw_cell,output_keep_prob=dropout_keep_prob)\n",
        "            lstm_bw_cell=tf.contrib.rnn.DropoutWrapper(lstm_bw_cell,output_keep_prob=dropout_keep_prob)\n",
        "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
        "                                                 dtype=tf.float32)\n",
        "\n",
        "    output_rnn=tf.concat(outputs,axis=2) #[batch_size,sequence_length,hidden_size*2]\n",
        "    output_rnn_last=tf.reduce_max(output_rnn,axis=1) #[batch_size,hidden_size*2] # this is average pooling\n",
        "    #output_rnn_last=output_rnn[:,-1,:] ##[batch_size,hidden_size*2] # this uses the last hidden state as the representation.\n",
        "    print(\"output_rnn_last:\", output_rnn_last)\n",
        "    return tf.matmul(output_rnn_last, weights) + biases\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "X = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "W = tf.Variable(tf.random_normal(shape=[2*num_hidden, n_classes]))\n",
        "b = tf.Variable(tf.zeros([n_classes]))\n",
        "\n",
        "output_logits = BiRNN(X, W, b, num_time_steps, num_hidden)\n",
        "y_pred = tf.nn.softmax(output_logits)\n",
        "\n",
        "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
        "\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(epochs):\n",
        "          X_batch, y_batch = next_batch(batch_size)\n",
        "          X_batch = X_batch.reshape((batch_size, num_time_steps, num_inputs))\n",
        "          y_batch = tf.one_hot(y_batch-1, n_classes, axis=-1).eval().reshape(-1, n_classes)\n",
        "\n",
        "          sess.run(optimizer, feed_dict={X: X_batch, y: y_batch})\n",
        "          if epoch % 100 == 0:\n",
        "              mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "              print(epoch, \"\\tMSE:\", mse)\n",
        "\n",
        "\n",
        "    X_batch_valid = np.asarray(validation_articles_w2v).reshape((len(validation_articles_w2v), num_time_steps, num_inputs))\n",
        "    y_batch_valid = tf.one_hot(validation_label_seq-1, n_classes, axis=-1).eval().reshape(-1, n_classes)\n",
        "\n",
        "    feed_dict_valid = {X: X_batch_valid, y: y_batch_valid}\n",
        "\n",
        "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
        "    print('---------------------------------------------------------')\n",
        "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
        "          format(epoch + 1, loss_valid, acc_valid))\n",
        "    print('---------------------------------------------------------')\n",
        "\n",
        "\n",
        "    '''y_pred = sess.run(cls_prediction, feed_dict={X: X_batch_valid[0].reshape(-1,num_time_steps, num_inputs)})\n",
        "    print(y_pred)\n",
        "    index = tf.argmax(y_pred, axis=0).eval()\n",
        "    print(index, y_batch_valid[0])'''\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output_rnn_last: Tensor(\"Max:0\", shape=(?, 400), dtype=float32)\n",
            "0 \tMSE: 6.1879497\n",
            "100 \tMSE: 0.008959124\n",
            "---------------------------------------------------------\n",
            "Epoch: 200, validation loss: 0.19, validation accuracy: 96.4%\n",
            "---------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VpCSQ5SDMI3v",
        "colab": {}
      },
      "source": [
        "'''model = tf.keras.Sequential([\n",
        "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
        "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
        "    # Add a Dense layer with 6 units and softmax activation.\n",
        "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "num_epochs = 10\n",
        "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}