{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis_net_poc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2url36uLJN/b/wC3A5B4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atis06/tensorflow_bilstm_pooling/blob/master/Thesis_net_poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDaWthyR4EE2",
        "colab_type": "code",
        "outputId": "47ae54c5-12b6-423f-9afa-48758da24807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!wget https://github.com/amirziai/cse6242-project/raw/master/resources/datasets/bbc-text.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-12 18:48:10--  https://github.com/amirziai/cse6242-project/raw/master/resources/datasets/bbc-text.csv\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/amirziai/cse6242-project/master/resources/datasets/bbc-text.csv [following]\n",
            "--2020-02-12 18:48:11--  https://raw.githubusercontent.com/amirziai/cse6242-project/master/resources/datasets/bbc-text.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5057493 (4.8M) [text/plain]\n",
            "Saving to: ‘bbc-text.csv’\n",
            "\n",
            "bbc-text.csv        100%[===================>]   4.82M  9.17MB/s    in 0.5s    \n",
            "\n",
            "2020-02-12 18:48:12 (9.17 MB/s) - ‘bbc-text.csv’ saved [5057493/5057493]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeH68YGY37hd",
        "colab_type": "code",
        "outputId": "971a00cf-d24a-4df9-af04-123b1c83c76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "vocab_size = 5000\n",
        "embedding_dim = 64\n",
        "max_length = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = 'OOV'\n",
        "training_portion = .8\n",
        "\n",
        "articles = []\n",
        "labels = []\n",
        "\n",
        "with open(\"bbc-text.csv\", 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        labels.append(row[0])\n",
        "        article = row[1]\n",
        "        for word in STOPWORDS:\n",
        "            token = ' ' + word + ' '\n",
        "            article = article.replace(token, ' ')\n",
        "            article = article.replace(' ', ' ')\n",
        "        articles.append(article)\n",
        "\n",
        "train_size = int(len(articles) * training_portion)\n",
        "\n",
        "train_articles = articles[0: train_size]\n",
        "train_labels = labels[0: train_size]\n",
        "\n",
        "validation_articles = articles[train_size:]\n",
        "validation_labels = labels[train_size:]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_articles)\n",
        "word_index = tokenizer.word_index\n",
        "dict(list(word_index.items())[0:10])\n",
        "\n",
        "\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
        "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_article(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_article(train_sequences[1]))\n",
        "print(decode_article(train_padded[1]))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "1.15.0\n",
            "worldcom boss left books alone former worldcom boss OOV ebbers accused OOV OOV £5 8bn fraud never made accounting decisions witness told jurors david OOV made comments questioning defence lawyers arguing mr ebbers responsible worldcom problems phone company collapsed 2002 prosecutors claim losses hidden protect firm shares mr OOV already pleaded guilty fraud OOV prosecutors monday defence lawyer OOV OOV tried distance client allegations cross OOV asked mr OOV ever knew mr ebbers make accounting decision aware mr OOV replied ever know mr ebbers make accounting entry worldcom books mr OOV OOV replied witness mr OOV admitted ordered false accounting entries request former worldcom chief financial officer scott sullivan defence lawyers trying OOV mr sullivan admitted fraud OOV later trial OOV behind worldcom accounting house cards mr ebbers team meanwhile looking OOV OOV boss OOV OOV OOV economist whatever OOV mr ebbers OOV worldcom OOV unknown OOV telecoms giant investor OOV late 1990s worldcom problems OOV however competition increased telecoms boom OOV out firm finally collapsed shareholders lost OOV 20 000 workers lost jobs mr ebbers trial expected last two months found guilty former OOV faces substantial jail sentence firmly declared OOV\n",
            "worldcom boss left books alone former worldcom boss OOV ebbers accused OOV OOV £5 8bn fraud never made accounting decisions witness told jurors david OOV made comments questioning defence lawyers arguing mr ebbers responsible worldcom problems phone company collapsed 2002 prosecutors claim losses hidden protect firm shares mr OOV already pleaded guilty fraud OOV prosecutors monday defence lawyer OOV OOV tried distance client allegations cross OOV asked mr OOV ever knew mr ebbers make accounting decision aware mr OOV replied ever know mr ebbers make accounting entry worldcom books mr OOV OOV replied witness mr OOV admitted ordered false accounting entries request former worldcom chief financial officer scott sullivan defence lawyers trying OOV mr sullivan admitted fraud OOV later trial OOV behind worldcom accounting house cards mr ebbers team meanwhile looking OOV OOV boss OOV OOV OOV economist whatever OOV mr ebbers OOV worldcom OOV unknown OOV telecoms giant investor OOV late 1990s worldcom problems OOV however competition increased telecoms boom OOV out firm finally collapsed shareholders lost OOV 20 000 workers lost jobs mr ebbers trial expected last two months found guilty former OOV faces substantial jail sentence firmly declared OOV ? ? ? ? ? ? ? ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cvnPLDe16hK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#W2V\n",
        "w2v_dim=300\n",
        "import gensim\n",
        "splitted_articles=[article.split() for article in articles]\n",
        "model = gensim.models.Word2Vec(iter=5, size=w2v_dim, workers=4,min_count=0) \n",
        "model.build_vocab(splitted_articles)\n",
        "model.train(splitted_articles, total_examples=len(splitted_articles), epochs = 15)\n",
        "\n",
        "vocabulary_size = len(model.wv.vocab)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJU0hnX1rtvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_articles_w2v = []\n",
        "\n",
        "for row in train_padded:\n",
        "  w2v_sentence = []\n",
        "  for word in enumerate(decode_article(row).split()):\n",
        "    try:\n",
        "      w2v_sentence.append(np.asarray(model.wv[word[1]]))\n",
        "    except Exception as e:\n",
        "      w2v_sentence.append(np.asarray(np.zeros(300)))\n",
        "  train_articles_w2v.append(np.asarray(w2v_sentence))\n",
        "\n",
        "#train_articles_w2v = np.asarray([elem[0:200] for elem in train_articles_w2v])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8c30WJX6ArF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def next_batch(batch_size):\n",
        "    idx = np.arange(0, len(train_articles_w2v))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[0:batch_size]\n",
        "    data_shuffle = [train_articles_w2v[i] for i in idx]\n",
        "    labels_shuffle = [training_label_seq[i] for i in idx]\n",
        "\n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
        "\n",
        "\n",
        "X_batch, y_batch = next_batch(40)\n",
        "print(X_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GARqFrnl_xDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_articles_w2v = []\n",
        "\n",
        "for row in validation_padded:\n",
        "  w2v_sentence = []\n",
        "  for word in enumerate(decode_article(row).split()):\n",
        "    try:\n",
        "      w2v_sentence.append(np.asarray(model.wv[word[1]]))\n",
        "    except Exception as e:\n",
        "      w2v_sentence.append(np.asarray(np.zeros(300)))\n",
        "  validation_articles_w2v.append(np.asarray(w2v_sentence))\n",
        "\n",
        "#train_articles_w2v = np.asarray([elem[0:200] for elem in train_articles_w2v])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9R-yMd04Miq",
        "colab_type": "code",
        "outputId": "744d8539-9934-426a-9e9e-f6b9307c80b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "######################################################################\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import LSTMCell, Bidirectional\n",
        "\n",
        "# Just one feature, the time series\n",
        "num_inputs = 300\n",
        "# Num of steps in each batch\n",
        "num_time_steps = 200\n",
        "# 100 neuron layer, play with this\n",
        "num_hidden = 200\n",
        "# Just one output, predicted time series\n",
        "num_outputs = 1\n",
        "# learning rate you can play with this\n",
        "learning_rate = 0.001\n",
        "# how many iterations to go through (training steps), you can play with this\n",
        "epochs = 200\n",
        "# Size of the batch of data\n",
        "batch_size = 100\n",
        "n_classes = 5\n",
        "\n",
        "'''with tf.Session() as sess:\n",
        "    print(tf.one_hot(y_batch, n_classes,\n",
        "                     axis=-1).eval())'''\n",
        "\n",
        "\n",
        "def RNN(x, weights, biases, timesteps, num_hidden):\n",
        "    x = tf.unstack(x, timesteps, 1)\n",
        "    rnn_cell = tf.contrib.rnn.BasicRNNCell(num_hidden)\n",
        "    states_series, current_state = tf.contrib.rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    return tf.matmul(states_series[-1], weights) + biases\n",
        "\n",
        "def BiRNN(x, weights, biases, timesteps, num_hidden):\n",
        "    x = tf.unstack(x, timesteps, 1)\n",
        "    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "    outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
        "                                                 dtype=tf.float32)\n",
        "    return tf.matmul(outputs[-1], weights) + biases\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "X = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "W = tf.Variable(tf.random_normal(shape=[2*num_hidden, n_classes]))\n",
        "b = tf.Variable(tf.zeros([n_classes]))\n",
        "\n",
        "output_logits = BiRNN(X, W, b, num_time_steps, num_hidden)\n",
        "y_pred = tf.nn.softmax(output_logits)\n",
        "\n",
        "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
        "\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(epochs):\n",
        "          X_batch, y_batch = next_batch(batch_size)\n",
        "          X_batch = X_batch.reshape((batch_size, num_time_steps, num_inputs))\n",
        "          y_batch = tf.one_hot(y_batch-1, n_classes, axis=-1).eval().reshape(-1, n_classes)\n",
        "\n",
        "          sess.run(optimizer, feed_dict={X: X_batch, y: y_batch})\n",
        "          if epoch % 100 == 0:\n",
        "              mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "              print(epoch, \"\\tMSE:\", mse)\n",
        "\n",
        "\n",
        "    X_batch_valid = np.asarray(validation_articles_w2v).reshape((len(validation_articles_w2v), num_time_steps, num_inputs))\n",
        "    y_batch_valid = tf.one_hot(validation_label_seq-1, n_classes, axis=-1).eval().reshape(-1, n_classes)\n",
        "\n",
        "    feed_dict_valid = {X: X_batch_valid, y: y_batch_valid}\n",
        "\n",
        "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
        "    print('---------------------------------------------------------')\n",
        "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
        "          format(epoch + 1, loss_valid, acc_valid))\n",
        "    print('---------------------------------------------------------')\n",
        "\n",
        "\n",
        "    '''y_pred = sess.run(cls_prediction, feed_dict={X: X_batch_valid[0].reshape(-1,num_time_steps, num_inputs)})\n",
        "    print(y_pred)\n",
        "    index = tf.argmax(y_pred, axis=0).eval()\n",
        "    print(index, y_batch_valid[0])'''\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 \tMSE: 1.4247719\n",
            "100 \tMSE: 0.49457473\n",
            "---------------------------------------------------------\n",
            "Epoch: 200, validation loss: 0.33, validation accuracy: 92.6%\n",
            "---------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5bebf17b-54a6-417f-8912-a33e94feb2ce",
        "id": "VpCSQ5SDMI3v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "'''model = tf.keras.Sequential([\n",
        "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
        "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
        "    # Add a Dense layer with 6 units and softmax activation.\n",
        "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "num_epochs = 10\n",
        "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-d477ba31ffe6>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)\u001b[0m\n\u001b[0m                                                                                                                                                  \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
          ]
        }
      ]
    }
  ]
}