{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis_net_poc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkx7tV0vQ3zr8DqKLqRBKO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atis06/tensorflow_bilstm_pooling/blob/master/Thesis_net_poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDaWthyR4EE2",
        "colab_type": "code",
        "outputId": "0d390d50-c4b9-43c9-88db-fb3a08a0d32d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!wget https://github.com/amirziai/cse6242-project/raw/master/resources/datasets/bbc-text.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-11 20:11:15--  https://github.com/amirziai/cse6242-project/raw/master/resources/datasets/bbc-text.csv\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/amirziai/cse6242-project/master/resources/datasets/bbc-text.csv [following]\n",
            "--2020-02-11 20:11:16--  https://raw.githubusercontent.com/amirziai/cse6242-project/master/resources/datasets/bbc-text.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5057493 (4.8M) [text/plain]\n",
            "Saving to: ‘bbc-text.csv’\n",
            "\n",
            "bbc-text.csv        100%[===================>]   4.82M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-02-11 20:11:16 (74.5 MB/s) - ‘bbc-text.csv’ saved [5057493/5057493]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeH68YGY37hd",
        "colab_type": "code",
        "outputId": "35c0e609-0e0c-4d6e-9fd0-ef164cf5767e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "vocab_size = 5000\n",
        "embedding_dim = 64\n",
        "max_length = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<UNK>'\n",
        "training_portion = .8\n",
        "\n",
        "articles = []\n",
        "labels = []\n",
        "\n",
        "with open(\"bbc-text.csv\", 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        labels.append(row[0])\n",
        "        article = row[1]\n",
        "        for word in STOPWORDS:\n",
        "            token = ' ' + word + ' '\n",
        "            article = article.replace(token, ' ')\n",
        "            article = article.replace(' ', ' ')\n",
        "        articles.append(article)\n",
        "print(len(labels))\n",
        "print(len(articles))\n",
        "\n",
        "train_size = int(len(articles) * training_portion)\n",
        "\n",
        "train_articles = articles[0: train_size]\n",
        "train_labels = labels[0: train_size]\n",
        "\n",
        "validation_articles = articles[train_size:]\n",
        "validation_labels = labels[train_size:]\n",
        "\n",
        "print(train_size)\n",
        "print(len(train_articles))\n",
        "print(len(train_labels))\n",
        "print(len(validation_articles))\n",
        "print(len(validation_labels))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_articles)\n",
        "word_index = tokenizer.word_index\n",
        "dict(list(word_index.items())[0:10])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
        "print(train_sequences[10])\n",
        "\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(len(train_sequences[0]))\n",
        "print(len(train_padded[0]))\n",
        "\n",
        "print(len(train_sequences[1]))\n",
        "print(len(train_padded[1]))\n",
        "\n",
        "print(len(train_sequences[10]))\n",
        "print(len(train_padded[10]))\n",
        "\n",
        "print(train_padded[10])\n",
        "\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
        "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(len(validation_sequences))\n",
        "print(validation_padded.shape)\n",
        "\n",
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
        "'''print(training_label_seq[0])\n",
        "print(training_label_seq[1])\n",
        "print(training_label_seq[2])\n",
        "print(training_label_seq.shape)\n",
        "\n",
        "print(validation_label_seq[0])\n",
        "print(validation_label_seq[1])\n",
        "print(validation_label_seq[2])\n",
        "print(validation_label_seq.shape)'''\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "\n",
        "def decode_article(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "\n",
        "print(decode_article(train_padded[10]))\n",
        "print('---')\n",
        "print(train_articles[10])\n",
        "\n",
        "print(train_padded.shape)\n",
        "print(train_labels[0])\n",
        "\n",
        "\n",
        "\n",
        "def next_batch(batch_size):\n",
        "    idx = np.arange(0, len(train_padded))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:batch_size]\n",
        "    data_shuffle = [train_padded[i] for i in idx]\n",
        "    labels_shuffle = [training_label_seq[i] for i in idx]\n",
        "\n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
        "\n",
        "\n",
        "X_batch, y_batch = next_batch(1)\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "1.15.0\n",
            "2225\n",
            "2225\n",
            "1780\n",
            "1780\n",
            "1780\n",
            "445\n",
            "445\n",
            "[2431, 1, 225, 4994, 22, 641, 587, 225, 4994, 1, 1, 1663, 1, 1, 2431, 22, 565, 1, 1, 140, 278, 1, 140, 278, 796, 822, 662, 2307, 1, 1144, 1695, 1, 1722, 4995, 1, 1, 1, 1, 1, 4737, 1, 1, 122, 4513, 1, 2, 2873, 1506, 352, 4738, 1, 52, 341, 1, 352, 2172, 3962, 41, 22, 3793, 1, 1, 1, 1, 543, 1, 1, 1, 835, 631, 2366, 347, 4739, 1, 365, 22, 1, 787, 2367, 1, 4301, 138, 10, 1, 3664, 682, 3531, 1, 22, 1, 414, 822, 662, 1, 90, 13, 633, 1, 225, 4994, 1, 600, 1, 1695, 1021, 1, 4996, 807, 1865, 117, 1, 1, 1, 2974, 22, 1, 99, 278, 1, 1607, 4997, 543, 492, 1, 1447, 4740, 778, 1320, 1, 1862, 10, 33, 641, 319, 1, 62, 478, 565, 301, 1507, 22, 479, 1, 1, 1666, 1, 797, 1, 3066, 1, 1365, 6, 1, 2431, 565, 22, 2971, 4734, 1, 1, 1, 1, 1, 850, 39, 1826, 675, 297, 26, 979, 1, 882, 22, 361, 22, 13, 301, 1507, 1343, 374, 20, 63, 883, 1096, 4302, 247]\n",
            "426\n",
            "200\n",
            "192\n",
            "200\n",
            "186\n",
            "200\n",
            "[2431    1  225 4994   22  641  587  225 4994    1    1 1663    1    1\n",
            " 2431   22  565    1    1  140  278    1  140  278  796  822  662 2307\n",
            "    1 1144 1695    1 1722 4995    1    1    1    1    1 4737    1    1\n",
            "  122 4513    1    2 2873 1506  352 4738    1   52  341    1  352 2172\n",
            " 3962   41   22 3793    1    1    1    1  543    1    1    1  835  631\n",
            " 2366  347 4739    1  365   22    1  787 2367    1 4301  138   10    1\n",
            " 3664  682 3531    1   22    1  414  822  662    1   90   13  633    1\n",
            "  225 4994    1  600    1 1695 1021    1 4996  807 1865  117    1    1\n",
            "    1 2974   22    1   99  278    1 1607 4997  543  492    1 1447 4740\n",
            "  778 1320    1 1862   10   33  641  319    1   62  478  565  301 1507\n",
            "   22  479    1    1 1666    1  797    1 3066    1 1365    6    1 2431\n",
            "  565   22 2971 4734    1    1    1    1    1  850   39 1826  675  297\n",
            "   26  979    1  882   22  361   22   13  301 1507 1343  374   20   63\n",
            "  883 1096 4302  247    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "445\n",
            "(445, 200)\n",
            "berlin <UNK> anti nazi film german movie anti nazi <UNK> <UNK> drawn <UNK> <UNK> berlin film festival <UNK> <UNK> final days <UNK> final days member white rose movement <UNK> 21 arrested <UNK> brother hans <UNK> <UNK> <UNK> <UNK> <UNK> tyranny <UNK> <UNK> director marc <UNK> said feeling responsibility keep legacy <UNK> going must <UNK> keep ideas alive added film drew <UNK> <UNK> <UNK> <UNK> trial <UNK> <UNK> <UNK> east germany secret police discovery <UNK> behind film <UNK> worked closely <UNK> relatives including one <UNK> sisters ensure historical <UNK> film <UNK> members white rose <UNK> group first started <UNK> anti nazi <UNK> summer <UNK> arrested dropped <UNK> munich university calling day <UNK> <UNK> <UNK> regime film <UNK> six days <UNK> arrest intense trial saw <UNK> initially deny charges ended <UNK> appearance one three german films <UNK> top prize festival south african film version <UNK> <UNK> opera <UNK> shot <UNK> town <UNK> language also <UNK> berlin festival film entitled u <UNK> <UNK> <UNK> <UNK> <UNK> story set performed 40 strong music theatre <UNK> debut film performance film first south african feature 25 years second nominated golden bear award ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
            "---\n",
            "berlin cheers anti-nazi film german movie anti-nazi resistance heroine drawn loud applause berlin film festival.  sophie scholl - final days portrays final days member white rose movement. scholl  21  arrested beheaded brother  hans  1943 distributing leaflets condemning  abhorrent tyranny  adolf hitler. director marc rothemund said:  feeling responsibility keep legacy scholls going.   must somehow keep ideas alive   added.  film drew transcripts gestapo interrogations scholl trial preserved archive communist east germany secret police. discovery inspiration behind film rothemund  worked closely surviving relatives  including one scholl sisters  ensure historical accuracy film. scholl members white rose resistance group first started distributing anti-nazi leaflets summer 1942. arrested dropped leaflets munich university calling  day reckoning  adolf hitler regime. film focuses six days scholl arrest intense trial saw scholl initially deny charges ended defiant appearance. one three german films vying top prize festival.  south african film version bizet tragic opera carmen shot cape town xhosa language also premiered berlin festival. film entitled u-carmen ekhayelitsha carmen khayelitsha township story set. performed 40-strong music theatre troupe debut film performance. film first south african feature 25 years second nominated golden bear award.\n",
            "(1780, 200)\n",
            "tech\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9R-yMd04Miq",
        "colab_type": "code",
        "outputId": "368db187-cfa5-4b27-a16f-e4efac1b56e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "######################################################################\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import LSTMCell, Bidirectional\n",
        "\n",
        "# Just one feature, the time series\n",
        "num_inputs = 200\n",
        "# Num of steps in each batch\n",
        "num_time_steps = 1\n",
        "# 100 neuron layer, play with this\n",
        "num_hidden = 5\n",
        "# Just one output, predicted time series\n",
        "num_outputs = 1\n",
        "# learning rate you can play with this\n",
        "learning_rate = 0.000001\n",
        "# how many iterations to go through (training steps), you can play with this\n",
        "epochs = 800\n",
        "# Size of the batch of data\n",
        "batch_size = 10\n",
        "n_classes = 5\n",
        "\n",
        "'''with tf.Session() as sess:\n",
        "    print(tf.one_hot(y_batch, n_classes,\n",
        "                     axis=-1).eval())'''\n",
        "\n",
        "\n",
        "def RNN(x, weights, biases, timesteps, num_hidden):\n",
        "    # Prepare data shape to match `rnn` function requirements\n",
        "    # Current data input shape: (batch_size, timesteps, n_input)\n",
        "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
        "\n",
        "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
        "    x = tf.unstack(x, timesteps, 1)\n",
        "\n",
        "    # Define a rnn cell with tensorflow\n",
        "    rnn_cell = tf.contrib.rnn.BasicRNNCell(num_hidden)\n",
        "\n",
        "    # Get lstm cell output\n",
        "    # If no initial_state is provided, dtype must be specified\n",
        "    # If no initial cell state is provided, they will be initialized to zero\n",
        "    states_series, current_state = tf.contrib.rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "\n",
        "    # Linear activation, using rnn inner loop last output\n",
        "    return tf.matmul(states_series[-1], weights) + biases\n",
        "\n",
        "def BiRNN(x, weights, biases, timesteps, num_hidden):\n",
        "    # Prepare data shape to match `rnn` function requirements\n",
        "    # Current data input shape: (batch_size, timesteps, n_input)\n",
        "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
        "\n",
        "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
        "    x = tf.unstack(x, timesteps, 1)\n",
        "\n",
        "    # Define lstm cells with tensorflow\n",
        "    # Forward direction cell\n",
        "    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "    # Backward direction cell\n",
        "    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "\n",
        "    # Get BiRNN cell output\n",
        "    outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
        "                                                 dtype=tf.float32)\n",
        "\n",
        "    # Linear activation, using rnn inner loop last output\n",
        "    return tf.matmul(outputs[-1], weights) + biases\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "X = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "W = tf.Variable(tf.random_normal(shape=[2*num_hidden, n_classes]))\n",
        "b = tf.Variable(tf.zeros([n_classes]))\n",
        "\n",
        "'''cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
        "    tf.contrib.rnn.BasicLSTMCell(num_units=num_hidden, activation=tf.nn.relu),\n",
        "    output_size=num_outputs)\n",
        "\n",
        "print(tf.unstack(X, num_time_steps, 1))\n",
        "\n",
        "outputs, state = tf.nn.dynamic_rnn(cell, tf.unstack(X, num_time_steps, 1), dtype=tf.float32)'''\n",
        "\n",
        "# output_logits = tf.matmul(state, W) + b\n",
        "output_logits = BiRNN(X, W, b, num_time_steps, num_hidden)\n",
        "y_pred = tf.nn.softmax(output_logits)\n",
        "\n",
        "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
        "\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        X_batch, y_batch = next_batch(batch_size)\n",
        "        # print(X_batch.shape)\n",
        "        X_batch = X_batch.reshape((batch_size, num_time_steps, num_inputs))\n",
        "        y_batch = tf.one_hot(y_batch-1, n_classes, axis=-1).eval().reshape(-1, n_classes)\n",
        "        \n",
        "        sess.run(optimizer, feed_dict={X: X_batch, y: y_batch})\n",
        "        if epoch % 100 == 0:\n",
        "            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            print(epoch, \"\\tMSE:\", mse)\n",
        "\n",
        "\n",
        "    X_batch_valid = validation_padded.reshape((len(validation_padded), num_time_steps, num_inputs))\n",
        "    y_batch_valid = tf.one_hot(validation_label_seq-1, n_classes, axis=-1).eval().reshape(-1, n_classes)\n",
        "\n",
        "    feed_dict_valid = {X: X_batch_valid, y: y_batch_valid}\n",
        "\n",
        "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
        "    print('---------------------------------------------------------')\n",
        "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
        "          format(epoch + 1, loss_valid, acc_valid))\n",
        "    print('---------------------------------------------------------')\n",
        "\n",
        "\n",
        "    y_pred = sess.run(cls_prediction, feed_dict={X: X_batch_valid[0].reshape(-1,1, num_inputs)})\n",
        "    print(y_pred)\n",
        "    index = tf.argmax(y_pred, axis=0).eval()\n",
        "    print(index, y_batch_valid[0])\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 \tMSE: 1.4226596\n",
            "100 \tMSE: 1.7893288\n",
            "200 \tMSE: 2.2262852\n",
            "300 \tMSE: 2.6442096\n",
            "400 \tMSE: 2.2937293\n",
            "500 \tMSE: 1.2811748\n",
            "600 \tMSE: 1.7587986\n",
            "700 \tMSE: 2.1783142\n",
            "---------------------------------------------------------\n",
            "Epoch: 800, validation loss: 1.90, validation accuracy: 22.5%\n",
            "---------------------------------------------------------\n",
            "[2]\n",
            "0 [0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "11b50731-fe53-4dc4-971f-3ada32e076e9",
        "id": "VpCSQ5SDMI3v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
        "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
        "    # Add a Dense layer with 6 units and softmax activation.\n",
        "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "num_epochs = 10\n",
        "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          320000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               66048     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 394,694\n",
            "Trainable params: 394,694\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 1780 samples, validate on 445 samples\n",
            "Epoch 1/10\n",
            "1780/1780 - 34s - loss: 1.5648 - acc: 0.2871 - val_loss: 1.2965 - val_acc: 0.3933\n",
            "Epoch 2/10\n",
            "1780/1780 - 34s - loss: 1.0321 - acc: 0.5747 - val_loss: 0.7455 - val_acc: 0.7506\n",
            "Epoch 3/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-87a1ac461211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_label_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_label_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}